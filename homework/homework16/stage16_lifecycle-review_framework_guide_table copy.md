# Applied Financial Engineering â€” Framework Guide

| Lifecycle Stage | What You Did | Challenges | Solutions / Decisions | Future Improvements |
|-----------------|--------------|------------|-----------------------|---------------------|
| **1. Problem Framing & Scoping** | Investigated how social media activity affects Carvana (CVNA) stock behavior. Goals included collecting stock and social data, analyzing patterns, and predicting movements. Key assumptions were the availability and reliability of data sources. | Difficult to precisely scope which social indicators to use. Ambiguity in causality between social media and stock price. | Defined success as finding predictive signals with reasonable accuracy. Focused on specific data windows matching stock market days. | Improve clarity by incorporating broader financial indicators and macroeconomic context. |
| **2. Tooling Setup** | Used Python with libraries such as pandas, yfinance, requests, BeautifulSoup for scraping, sklearn for modeling. Set up environment with Anaconda and VS Code. | Initial missing packages (dotenv) caused environment errors. API key management was a challenge. | Installed missing packages promptly; used fallback API calls to yfinance. Used `.env` file for keys but noted validation issues. | Automate environment setup and key management with scripts and documentation. |
| **3. Python Fundamentals** | Applied core Python for data ingestion, preprocessing, scraping, and basic modeling. Used pandas extensively for data frames. | Learned more about web scraping nuances and API handling. Coding fluency gaps in pipeline automation. | Closed gaps through practice and debugging sessions. Reused snippets from official docs and examples. | Strengthen skills in modular code design and pipeline automation. |
| **4. Data Acquisition / Ingestion** | Ingested stock price data from Finnhub API with fallback to yfinance. Scraped financial summaries from Finviz. Generated synthetic social media data using simulated realistic distributions. | API key for Finnhub was invalid, causing failed direct downloads. Scraping Yahoo Finance had no tables returned due to frontend changes. | Used fallback data source yfinance and alternative scraping from Finviz. Generated representative social data artificially. | Improve robustness by adding more fallback data sources and scraping strategies. |
| **5. Data Storage** | Stored raw data as CSV files under `/data/raw/`. Processed datasets saved under `/data/processed/`. | Handling different data formats and schemas was tricky. Ensuring consistent timestamps between stock and social data. | Used consistent naming conventions and timestamp formatting. | Automate schema validation and add metadata tracking. |
| **6. Data Preprocessing** | Cleaned and merged stock and social datasets, handled missing values, normalized numeric ranges. | Missing data points and API inconsistencies made cleaning nontrivial. | Used pandas to impute or drop missing values selectively. Aligned dates across datasets. | Use more advanced imputation techniques and anomaly detection. |
| **7. Outlier Analysis** | Detected volatility days and volume spikes in stock data, checked for unusual social media surges. | Differentiating genuine market moves versus data errors was challenging. | Flagged anomalies with domain knowledge heuristics and statistical thresholds. | Apply robust statistical outlier detection and visualization. |
| **8. Exploratory Data Analysis (EDA)** | Visualized stock price trends, volume, social media post counts, correlations, and feature distributions. | Some correlations weak or nonintuitive, interpretation required care. | Used best-fit lines and rolling statistics to clarify patterns. | Add interactive dashboards for stakeholder presentations. |
| **9. Feature Engineering** | Created lagged returns, volatility indicators, social media momentum features. | Designing features that could realistically capture social sentiment influence was complex. | Validated features by checking correlations and predictive power. | Incorporate natural language processing on actual social media text data. |
| **10. Modeling (Regression / Time Series / Classification)** | Tested regression and classification models including Linear Regression and Random Forest to predict price movement. | Risk of overfitting due to limited feature sets. | Used cross-validation time splits, regularization, and feature selection. | Explore deep learning models and ensemble methods. |
| **11. Evaluation & Risk Communication** | Used RMSE, MAE, accuracy and confusion matrices. Communicated risks in model uncertainty and data quality. | Conveying scientific rigor while keeping it understandable to stakeholders was hard. | Balanced technical reports with executive summaries. | Improve probabilistic uncertainty quantification. |
| **12. Results Reporting, Delivery Design & Stakeholder Communication** | Delivered reports and Jupyter notebooks with graphs and commentary explaining analytical insights. | Simplifying jargon for non-technical audiences was challenging. | Used visual storytelling and clear explanations. | Develop slide decks and short video summaries. |
| **13. Productization** | Packaged data ingestion and preprocessing steps into reusable Python functions within `/src/`. | Scaling pipelines for larger datasets was not fully addressed. | Designed modular, parameterized ingestion scripts. | Add automated testing and continuous integration. |
| **14. Deployment & Monitoring** | No live deployment yet; planning for monitoring data pipeline health and model drift. | Lack of real-time data limits deployment options. | Scheduled batch updates and manual performance tracking. | Implement automated alerts and dashboard monitoring. |
| **15. Orchestration & System Design** | Integrated data ingestion, processing, and modeling scripts in workflow notebooks. | Dependency management across scripts was manual. | Used consistent file paths and environment variables. | Implement workflow orchestration tools (e.g., Airflow, Prefect). |
| **16. Lifecycle Review & Reflection** | Reflected on data challenges, tool limitations, and importance of robust pipelines. | API access and scraping were major hurdles. | Emphasized fallback strategies and code reuse. | Plan better early scoping and deeper domain collaboration. |
