{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup: Generate Sample Dataset\n",
    "\n",
    "This cell creates the required folder structure (`data/raw/` and `data/processed/`) relative to the notebook, and generates the sample CSV dataset with missing values. \n",
    "This ensures the dataset is ready for cleaning functions and saves it to `data/raw/sample_data.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists at ../data/raw\\sample_data.csv. Skipping CSV creation to avoid overwrite.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define folder paths relative to this notebook\n",
    "raw_dir = '../data/raw'\n",
    "processed_dir = '../data/processed'\n",
    "\n",
    "# Create folders if they don't exist\n",
    "os.makedirs(raw_dir, exist_ok=True)\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "# Define the sample data\n",
    "data = {\n",
    "    'age': [34, 45, 29, 50, 38, np.nan, 41],\n",
    "    'income': [55000, np.nan, 42000, 58000, np.nan, np.nan, 49000],\n",
    "    'score': [0.82, 0.91, np.nan, 0.76, 0.88, 0.65, 0.79],\n",
    "    'zipcode': ['90210', '10001', '60614', '94103', '73301', '12345', '94105'],\n",
    "    'city': ['Beverly', 'New York', 'Chicago', 'SF', 'Austin', 'Unknown', 'San Francisco'],\n",
    "    'extra_data': [np.nan, 42, np.nan, np.nan, np.nan, 5, np.nan]\n",
    "}\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save to CSV in raw data folder\n",
    "csv_path = os.path.join(raw_dir, 'sample_data.csv')\n",
    "if not os.path.exists(csv_path):\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f'Sample dataset created and saved to {csv_path}')\n",
    "else:\n",
    "    print(f'File already exists at {csv_path}. Skipping CSV creation to avoid overwrite.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Starter â€” Stage 6: Data Preprocessing\n",
    "Use this notebook to apply your cleaning functions and save processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully!\n",
      "Original Data:\n",
      "    age   income  score  zipcode      city  extra_data\n",
      "0  34.0  55000.0   0.82    90210   Beverly         NaN\n",
      "1  45.0      NaN   0.91    10001  New York        42.0\n",
      "2  29.0  42000.0    NaN    60614   Chicago         NaN\n",
      "3  50.0  58000.0   0.76    94103        SF         NaN\n",
      "4  38.0      NaN   0.88    73301    Austin         NaN\n",
      "\n",
      "Cleaned Data:\n",
      "    age   income  score  zipcode      city  extra_data\n",
      "0  34.0  55000.0  0.820    90210   beverly        23.5\n",
      "1  45.0  52000.0  0.910    10001  new york        42.0\n",
      "2  29.0  42000.0  0.805    60614   chicago        23.5\n",
      "3  50.0  58000.0  0.760    94103        sf        23.5\n",
      "4  38.0  52000.0  0.880    73301    austin        23.5\n",
      "\n",
      "Cleaned data saved to c:\\Users\\kavin\\bootcamp_Kavin_Dhanasekaran\\homework\\homework6\\data/raw/sample_data_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Determine project root\n",
    "try:\n",
    "    # Works if running as a script\n",
    "    project_root = os.path.abspath(os.path.dirname(__file__))\n",
    "except NameError:\n",
    "    # Works in Jupyter notebooks\n",
    "    project_root = os.path.abspath(os.getcwd())\n",
    "\n",
    "# Ensure project root is in sys.path\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Import cleaning function\n",
    "from src.cleaning import clean_data\n",
    "\n",
    "# Load CSV\n",
    "file_path = os.path.join(project_root, 'data/raw/sample_data.csv')\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(\"Data loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {file_path}\")\n",
    "    exit(1)\n",
    "\n",
    "# Show original data\n",
    "print(\"Original Data:\")\n",
    "print(df.head())\n",
    "\n",
    "# Clean data\n",
    "df_cleaned = clean_data(df)\n",
    "print(\"\\nCleaned Data:\")\n",
    "print(df_cleaned.head())\n",
    "\n",
    "# Optionally save cleaned data\n",
    "cleaned_file = os.path.join(project_root, 'data/raw/sample_data_cleaned.csv')\n",
    "df_cleaned.to_csv(cleaned_file, index=False)\n",
    "print(f\"\\nCleaned data saved to {cleaned_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Raw Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_median(df, columns):\n",
    "    \"\"\"\n",
    "    Fills missing values in specified columns with the column median.\n",
    "    \n",
    "    Parameters:\n",
    "        df (DataFrame): Input pandas DataFrame.\n",
    "        columns (list): List of column names to fill.\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with missing values filled.\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            median = df[col].median()\n",
    "            df[col].fillna(median, inplace=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_missing(df, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Drops rows with a proportion of missing values exceeding the threshold.\n",
    "    \"\"\"\n",
    "    return df[df.isnull().mean(axis=1) <= threshold]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(df, columns):\n",
    "    \"\"\"\n",
    "    Normalizes specified columns using min-max scaling.\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            min_val = df[col].min()\n",
    "            max_val = df[col].max()\n",
    "            # Avoid division by zero\n",
    "            if pd.notnull(min_val) and pd.notnull(max_val) and max_val != min_val:\n",
    "                df[col] = (df[col] - min_val) / (max_val - min_val)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "def drop_missing(df: pd.DataFrame, how: str = 'all', axis: int = 0, threshold: float = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Drop rows or columns with missing values.\n",
    "\n",
    "    Parameters:\n",
    "        df: pd.DataFrame\n",
    "        how: 'all' or 'any' (used if threshold is None)\n",
    "        axis: 0 = rows, 1 = columns\n",
    "        threshold: float (0-1), minimum proportion of non-NA values required to keep row/column\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame with rows/columns dropped\n",
    "    \"\"\"\n",
    "    if threshold is not None:\n",
    "        min_count = int((1 - threshold) * df.shape[axis])\n",
    "        return df.dropna(axis=axis, thresh=min_count)\n",
    "    else:\n",
    "        return df.dropna(how=how, axis=axis)\n",
    "\n",
    "\n",
    "def fill_missing_median(df: pd.DataFrame, columns=None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fill missing numeric values with the median of their column.\n",
    "    \n",
    "    Parameters:\n",
    "        df: pd.DataFrame\n",
    "        columns: list of numeric columns to fill. If None, fill all numeric columns.\n",
    "    \"\"\"\n",
    "    if columns is None:\n",
    "        columns = [col for col in df.columns if pd.api.types.is_numeric_dtype(df[col])]\n",
    "    \n",
    "    for col in columns:\n",
    "        if col in df.columns and pd.api.types.is_numeric_dtype(df[col]):\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def normalize_data(df: pd.DataFrame, columns=None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Normalize string/categorical columns by stripping whitespace\n",
    "    and converting to lowercase. Fill missing string values with 'Unknown'.\n",
    "    \n",
    "    Parameters:\n",
    "        df: pd.DataFrame\n",
    "        columns: list of columns to normalize. If None, normalize all non-numeric columns.\n",
    "    \"\"\"\n",
    "    if columns is None:\n",
    "        columns = [col for col in df.columns if not pd.api.types.is_numeric_dtype(df[col])]\n",
    "    \n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna('Unknown').astype(str).str.strip().str.lower()\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_duplicates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove duplicate rows from the DataFrame.\n",
    "    \"\"\"\n",
    "    return df.drop_duplicates()\n",
    "\n",
    "\n",
    "def clean_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Full cleaning pipeline:\n",
    "      1. Drop rows where all values are missing\n",
    "      2. Remove duplicates\n",
    "      3. Fill missing numeric values\n",
    "      4. Normalize text columns\n",
    "    \"\"\"\n",
    "    df = drop_missing(df, how='all')\n",
    "    df = remove_duplicates(df)\n",
    "    df = fill_missing_median(df)\n",
    "    df = normalize_data(df)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    age   income  score  zipcode      city  extra_data\n",
      "0  34.0  55000.0  0.820    90210   beverly        23.5\n",
      "1  45.0  52000.0  0.910    10001  new york        42.0\n",
      "2  29.0  42000.0  0.805    60614   chicago        23.5\n",
      "3  50.0  58000.0  0.760    94103        sf        23.5\n",
      "4  38.0  52000.0  0.880    73301    austin        23.5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(r\"C:\\Users\\kavin\\bootcamp_Kavin_Dhanasekaran\\homework\\homework6\\data\\raw\\sample_data.csv\")\n",
    "\n",
    "# Individual operations\n",
    "df = drop_missing(df, how='all')\n",
    "df = remove_duplicates(df)\n",
    "df = fill_missing_median(df)\n",
    "df = normalize_data(df)\n",
    "\n",
    "# Or run full pipeline\n",
    "df_cleaned = clean_data(df)\n",
    "\n",
    "print(df_cleaned.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Apply your functions here\n",
    "# Example:\n",
    "# df = cleaning.fill_missing_median(df, ['col1','col2'])\n",
    "# df = cleaning.drop_missing(df, threshold=0.5)\n",
    "# df = cleaning.normalize_data(df, ['col1','col2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Cleaned Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('../data/processed/sample_data_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaning and processing complete. Cleaned data saved to '../data/processed\\sample_data_cleaned.csv'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Ensure the output directory exists\n",
    "output_dir = '../data/processed'\n",
    "os.makedirs(output_dir, exist_ok=True)  # Creates the folder if it doesn't exist\n",
    "\n",
    "# Save processed dataset\n",
    "output_file = os.path.join(output_dir, 'sample_data_cleaned.csv')\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"Data cleaning and processing complete. Cleaned data saved to '{output_file}'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datacleaning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
